{
  "title": "AI Prompt Token Counter",
  "description": "Count tokens for AI prompts across GPT-4, Claude, Llama models with accurate cost estimation",
  "placeholder": "Enter your prompt or paste text here...",
  "meta": {
    "title": "AI Token Counter - GPT-4, Claude, Llama Token Calculator | ToolsLab",
    "description": "Free AI token counter and cost calculator. Count tokens for GPT-4, GPT-3.5, Claude, Llama, Gemini, and Mistral models. Estimate API costs, optimize prompts, and compare models. Perfect for prompt engineers and AI developers."
  },
  "tagline": "Count tokens and estimate costs across GPT-4, Claude, Llama, and more AI models",
  "pageDescription": "Professional AI token counter for prompt engineers and developers. Accurately count tokens for OpenAI GPT-4, GPT-3.5 Turbo, Claude 3/4, Llama 2/3, Gemini, and Mistral models. Get detailed cost estimates with latest API pricing, compare token counts across all models, and optimize your prompts to reduce tokens and costs. Features real-time token counting, context window tracking, batch analysis, prompt optimization suggestions, and comprehensive cost projections. Save your analysis history and export results as JSON. All processing happens locally in your browser for maximum privacy and speed.",
  "instructions": {
    "title": "How to use AI Token Counter",
    "steps": [
      {
        "title": "Enter your prompt or text",
        "description": "Type or paste your AI prompt into the large text area. You can also upload .txt or .md files by clicking the upload button or dragging and dropping them. The tool supports very long texts (100k+ characters) and auto-saves your input to prevent data loss."
      },
      {
        "title": "Select your AI model",
        "description": "Choose the AI model you're using from the dropdown: GPT-4, GPT-4 Turbo, GPT-3.5, Claude 3/3.5/4, Llama 2/3, Gemini Pro/1.5, or Mistral. Each model uses a different tokenizer and has different context windows and pricing. The tool shows the model's context limit and tokenizer type."
      },
      {
        "title": "View token count and analysis",
        "description": "See instant results including total tokens, characters, words, token-to-word ratio, and context window usage. A visual progress bar shows how much of the model's context you're using (green: 0-50%, yellow: 50-80%, red: 80-100%). Get warnings if you're approaching the limit and suggestions for optimization."
      },
      {
        "title": "Estimate API costs",
        "description": "View detailed cost estimates based on the latest API pricing (updated January 2025). See costs per request, per 100 requests, per 1,000, and per 10,000 requests. Adjust the output token ratio slider to estimate costs including expected model responses. Pricing includes both input and output token costs."
      },
      {
        "title": "Compare models and optimize",
        "description": "Switch to Compare view to see token counts and costs across all models simultaneously. Use Optimize view to get an optimized version of your prompt with reduced tokens. The tool removes excessive whitespace, repetitive patterns, and unnecessary formatting while preserving meaning."
      }
    ],
    "features": [
      "Support for 15+ AI models including GPT-4o, Claude 3.5, Llama 3.1, Gemini 2.0, Mistral",
      "Accurate token estimation for each model's specific tokenizer",
      "Real-time token counting as you type (debounced for performance)",
      "Context window tracking with visual progress indicator",
      "Comprehensive cost estimation with latest API pricing (January 2025)",
      "Multi-model comparison to find the most cost-effective option",
      "Prompt optimization to reduce token count",
      "Token-to-word ratio analysis for efficiency insights",
      "Batch processing for multiple prompts",
      "Automatic history saving of last 20 analyses",
      "Auto-save input to prevent data loss",
      "File upload support (.txt, .md) with drag & drop",
      "Export results as JSON with full analysis data",
      "Copy results to clipboard",
      "Warnings when approaching context limits",
      "Optimization suggestions for token reduction",
      "Cost projections for scaled usage (100, 1k, 10k requests)",
      "Works 100% offline after initial load",
      "No data sent to external servers"
    ],
    "useCases": [
      "Estimate API costs before deploying AI applications",
      "Optimize prompts to reduce token usage and costs",
      "Choose the most cost-effective model for your use case",
      "Verify prompts fit within model context windows",
      "Analyze token efficiency of different prompt styles",
      "Budget planning for AI API usage",
      "Test prompt variations to find the most token-efficient version",
      "Calculate costs for batch processing scenarios",
      "Compare tokenization differences across models",
      "Optimize long-form content for AI processing",
      "Debug why prompts are being truncated",
      "Plan context allocation for RAG applications",
      "Estimate costs for AI-powered features before development",
      "Monitor token usage trends in prompt engineering workflows"
    ],
    "proTips": [
      "Use GPT-3.5 for simple tasks - it's 20x cheaper than GPT-4 with similar tokenization",
      "Claude models are very efficient for long contexts - use them for documents",
      "Check token-to-word ratio: values > 2 indicate inefficient formatting",
      "Remove markdown formatting if not needed - it adds extra tokens",
      "Use the Optimize feature before finalizing production prompts",
      "Compare models in the Compare view to find cost savings",
      "Adjust output token ratio to estimate full conversation costs",
      "Save frequently used prompts in history for quick access",
      "Upload files directly instead of copy-pasting to handle very long texts",
      "Context at 80%+ means the model might truncate responses - split your prompt",
      "Gemini 2.0 Flash has 1M token context - excellent for long documents",
      "For code generation, GPT-4 Turbo offers best balance of quality and cost"
    ],
    "troubleshooting": [
      "Token counts are estimates: Real API counts may vary slightly (Â±5%) due to exact tokenizer implementations. Use this tool for planning and optimization, not exact billing.",
      "Context limit warnings: If you see red (80%+), your prompt may be truncated. Split it into multiple requests or use a model with larger context window.",
      "High token-to-word ratio: Usually caused by special characters, code, or excessive formatting. The Optimize feature can help reduce this.",
      "Pricing discrepancies: Prices are updated regularly but check official provider documentation for latest rates. Last updated: January 2025.",
      "File upload issues: Only .txt and .md files are supported. For other formats, copy the text content and paste it.",
      "History not saving: Enable localStorage in your browser. History is stored locally and never sent to servers.",
      "Slow performance with very long texts: Token counting for 50k+ character texts may take a few seconds. This is normal and happens in your browser."
    ],
    "keyboardShortcuts": [
      {
        "keys": "Ctrl+V",
        "description": "Paste text"
      },
      {
        "keys": "Ctrl+A",
        "description": "Select all text"
      },
      {
        "keys": "Ctrl+C",
        "description": "Copy selected"
      }
    ]
  }
}
